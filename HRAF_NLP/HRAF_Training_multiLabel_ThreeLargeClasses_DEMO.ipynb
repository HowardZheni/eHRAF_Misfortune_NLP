{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "from datasets.dataset_dict import DatasetDict\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font color=\"red\">NOTE</font> This code is outdated and a new model shall be created. Culture_Coding.xlsx files have been changed to _Altogether_Dataset_RACoded.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108cc7dbf90b45fd9a9e021e3b90c36f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# OPTIONAL IF YOU WANT TO PUSH TO THE HUB!\n",
    "from huggingface_hub import notebook_login\n",
    "# IF RUNNING THIS CELL DOES NOT WORK:\n",
    "# copy and paste this code in the terminal: huggingface-cli login \n",
    "# then paste this token: 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# It may not show up but still paste the token in and press enter\n",
    "\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"10\" halign=\"left\">CULTURE</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"6\" halign=\"left\">ACTION</th>\n",
       "      <th>OTHER</th>\n",
       "      <th colspan=\"3\" halign=\"left\">CODER</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Passage Number</th>\n",
       "      <th>Region</th>\n",
       "      <th>SubRegion</th>\n",
       "      <th>Culture</th>\n",
       "      <th>DocTitle</th>\n",
       "      <th>Section</th>\n",
       "      <th>Author</th>\n",
       "      <th>Page</th>\n",
       "      <th>Year</th>\n",
       "      <th>OCM</th>\n",
       "      <th>...</th>\n",
       "      <th>Divination</th>\n",
       "      <th>Shaman_Medium_Healer</th>\n",
       "      <th>Priest_High_Religion</th>\n",
       "      <th>Other</th>\n",
       "      <th>Description</th>\n",
       "      <th>Local_terms</th>\n",
       "      <th>Other_Comments</th>\n",
       "      <th>Run_Number</th>\n",
       "      <th>Finished</th>\n",
       "      <th>Coder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>22570</td>\n",
       "      <td>Asia</td>\n",
       "      <td>East Asia</td>\n",
       "      <td>Korea</td>\n",
       "      <td>Ancestor worship and Korean society</td>\n",
       "      <td>Entering a Husbandâ€™s Family</td>\n",
       "      <td>Janelli, Roger L.</td>\n",
       "      <td>41</td>\n",
       "      <td>1982</td>\n",
       "      <td>['607', '753', '756']</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>the younger brother's wife of man who became i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>AH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>15572</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Southeastern Europe</td>\n",
       "      <td>Serbs</td>\n",
       "      <td>A Serbian village</td>\n",
       "      <td>THE CHRISTMAS SEASON</td>\n",
       "      <td>Halpern, Joel Martin</td>\n",
       "      <td>244</td>\n",
       "      <td>1967</td>\n",
       "      <td>['574', '777', '789', '796']</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>This is counteracted by having someone from wi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>AH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>15569</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Southeastern Europe</td>\n",
       "      <td>Serbs</td>\n",
       "      <td>A Serbian village</td>\n",
       "      <td>SOME BELIEFS AND CUSTOMS</td>\n",
       "      <td>Halpern, Joel Martin</td>\n",
       "      <td>242</td>\n",
       "      <td>1967</td>\n",
       "      <td>['563', '731', '789', '821']</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>no misfortune stated, continued from 3 passage...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>AH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           CULTURE                                        \n",
       "    Passage Number  Region            SubRegion Culture   \n",
       "198          22570    Asia            East Asia   Korea  \\\n",
       "410          15572  Europe  Southeastern Europe   Serbs   \n",
       "409          15569  Europe  Southeastern Europe   Serbs   \n",
       "\n",
       "                                                                        \n",
       "                                DocTitle                      Section   \n",
       "198  Ancestor worship and Korean society  Entering a Husbandâ€™s Family  \\\n",
       "410                    A Serbian village         THE CHRISTMAS SEASON   \n",
       "409                    A Serbian village     SOME BELIEFS AND CUSTOMS   \n",
       "\n",
       "                                                                    ...   \n",
       "                   Author Page  Year                           OCM  ...   \n",
       "198     Janelli, Roger L.   41  1982         ['607', '753', '756']  ...  \\\n",
       "410  Halpern, Joel Martin  244  1967  ['574', '777', '789', '796']  ...   \n",
       "409  Halpern, Joel Martin  242  1967  ['563', '731', '789', '821']  ...   \n",
       "\n",
       "        ACTION                                                   \n",
       "    Divination Shaman_Medium_Healer Priest_High_Religion Other   \n",
       "198          0                    1                    0     1  \\\n",
       "410          0                    0                    0     0   \n",
       "409          0                    0                    0     0   \n",
       "\n",
       "                                                                     \n",
       "                                           Description Local_terms   \n",
       "198  the younger brother's wife of man who became i...           0  \\\n",
       "410  This is counteracted by having someone from wi...           0   \n",
       "409                                                  0           0   \n",
       "\n",
       "                                                 OTHER      CODER            \n",
       "                                        Other_Comments Run_Number Finished   \n",
       "198                                                  0          1     True  \\\n",
       "410                                                  0          1    False   \n",
       "409  no misfortune stated, continued from 3 passage...          1    False   \n",
       "\n",
       "           \n",
       "    Coder  \n",
       "198    AH  \n",
       "410    AH  \n",
       "409    AH  \n",
       "\n",
       "[3 rows x 40 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../../../eHRAF_Scraper-Analysis-and-Prep/Data/\"\n",
    "dataFolder = r\"(subjects-(contracts_OR_disabilities_OR_disasters_OR_friendships_OR_gift_giving_OR_infant_feeding_OR_lineages_OR_local_officials_OR_luck_and_chance_OR_magicians_and_diviners_OR_mortuary_specialists_OR_nuclear_family_OR_priesthood_OR_prophet/\"\n",
    "# dataFolder = r'subjects-(sickness)_FILTERS-culture_level_samples(PSF)'\n",
    "\n",
    "df = pd.read_excel(f\"{path}{dataFolder}/_Altogether_Dataset_RACoded.xlsx\", header=[0,1], index_col=0)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ID', 'passage', 'EVENT', 'CAUSE', 'ACTION'],\n",
       "        num_rows: 2910\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['ID', 'passage', 'EVENT', 'CAUSE', 'ACTION'],\n",
       "        num_rows: 728\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# subdivide into just passage and outcome\n",
    "df_small = pd.DataFrame()\n",
    "df_small[[\"ID\",\"passage\",\"EVENT\",\"CAUSE\",\"ACTION\"]] = df[[('CULTURE', \"Passage Number\"), ('CULTURE', \"Passage\"), ('EVENT', \"No_Info\"), ('CAUSE', \"No_Info\"), ('ACTION', \"No_Info\")]]\n",
    "# Flip the lable of \"no_info\"\n",
    "df_small[[\"EVENT\",\"CAUSE\",\"ACTION\"]]  = df_small[[\"EVENT\",\"CAUSE\",\"ACTION\"]].replace({0:1, 1:0})\n",
    "\n",
    "# Remove certain passages which should not be in training or inference (these are duplicates that had to be manually found by a human)\n",
    "values_to_remove = [3252, 33681, 6758, 10104]\n",
    "df_small = df_small[~df_small['ID'].isin(values_to_remove)]\n",
    "df_small\n",
    "\n",
    "\n",
    "# # create train and validation/test sets\n",
    "# train_val, test = train_test_split(df_small, test_size=0.2, random_state=10)\n",
    "\n",
    "# create train and validation/test sets\n",
    "train_val, test = train_test_split(df_small, test_size=0.2, random_state=10)\n",
    "# # do it again to get the test and validation sets (15% = 50% * 30%)\n",
    "# test, validation = train_test_split(test_val, test_size=0.5, random_state=10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create an NLP friendly dataset\n",
    "Hraf = DatasetDict(\n",
    "    {'train':Dataset.from_dict(train_val.to_dict(orient= 'list')),\n",
    "     'test':Dataset.from_dict(test.to_dict(orient= 'list'))})\n",
    "Hraf\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save partitioned dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir(path):\n",
    "    import os\n",
    "    # Check whether the specified path exists or not\n",
    "    isExist = os.path.exists(path)\n",
    "    if not isExist:\n",
    "    # Create a new directory because it does not exist\n",
    "        os.makedirs(path)\n",
    "\n",
    "# make folder if it does not exist yet\n",
    "path = os.getcwd() + '/Datasets'\n",
    "make_dir(path)\n",
    "# save to Json\n",
    "for key in Hraf.keys():\n",
    "    Hraf_dict = Hraf[key].to_dict()\n",
    "    with open(f\"{path}/{key}_dataset.json\", \"w\") as outfile:\n",
    "        json.dump(Hraf_dict, outfile)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the training set is as biased as our groups (we want to train on as or less biased data as the groups they come from) <br>\n",
    "We are shooting for .85, .68 and .68 for EVENT, CAUSE, and ACTION respectively\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                TOTAL     train     test\n",
      "____________________________________________________________\n",
      "EVENT:                          61.96     61.79     62.64     \n",
      "CAUSE:                          47.2      46.94     48.21     \n",
      "ACTION:                         47.91     48.45     45.74     \n"
     ]
    }
   ],
   "source": [
    "# extract the total proportion\n",
    "def totalProportion(df, col, present=1):\n",
    "    value_counts = df[col].value_counts()\n",
    "    percentage = round(value_counts[present]/len(df)*100,2)\n",
    "    return percentage\n",
    "\n",
    "# extracts percentages per datafaframe\n",
    "def colProportion(Hraf, col):\n",
    "    percentage_list = []\n",
    "    for dataframe in Hraf.keys():\n",
    "        percentage_list += [round(sum(Hraf[dataframe][col]) / (len(Hraf[dataframe]))*100,2)]\n",
    "    return percentage_list\n",
    "\n",
    "\n",
    "\n",
    "# print bias per label\n",
    "dataframe_keys= Hraf.keys()\n",
    "labels = [label for label in Hraf['train'].features.keys() if label not in ['ID', 'passage']]\n",
    "header = \"                                TOTAL\"\n",
    "for key in dataframe_keys:\n",
    "    header += f\"     {key}\"\n",
    "print(header)\n",
    "print('_'*(len(header)+4))\n",
    "for col in labels:\n",
    "    totalPercentage =  totalProportion(df_small, col)\n",
    "    percentage_list =  colProportion(Hraf, col)\n",
    "    spacing = 10\n",
    "    percentage_str = f\"{totalPercentage}{' '* (spacing-len(str(totalPercentage)))}\"\n",
    "    for index, key in enumerate(dataframe_keys):\n",
    "        percentage_str += f\"{(len(key)-5)*' '}{percentage_list[index]}{' '* (spacing-len(str(percentage_list[index])))}\"\n",
    "    print(f\"{col}:{' ' * (30- len(col))} {percentage_str}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create labels for training and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'EVENT', 1: 'CAUSE', 2: 'ACTION'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "labels = [label for label in Hraf['train'].features.keys() if label not in ['ID', 'passage']]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "id2label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load a DistilBERT tokenizer to preprocess the text field: <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a preprocessing function to tokenize text and truncate sequences to be no longer than DistilBERTâ€™s maximum input length:<br>\n",
    "Guidelines were followed from NielsRogge found <a href= \"https://github.com/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb\"> here </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_data(examples):\n",
    "  # take a batch of texts\n",
    "  text = examples[\"passage\"]\n",
    "  # encode them\n",
    "  encoding = tokenizer(text, max_length=512, truncation=True) #max length for BERT is 512\n",
    "  # add labels\n",
    "  labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "  # create numpy array of shape (batch_size, num_labels)\n",
    "  labels_matrix = np.zeros((len(text), len(labels)))\n",
    "  # fill numpy array\n",
    "  for idx, label in enumerate(labels):\n",
    "    labels_matrix[:, idx] = labels_batch[label]\n",
    "\n",
    "  encoding[\"labels\"] = labels_matrix.tolist()\n",
    "\n",
    "  return encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the preprocessing function over the entire dataset, use ðŸ¤— Datasets map function. You can speed up map by setting batched=True to process multiple elements of the dataset at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5129b235676d4e0dab370b0a72ca2622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2910 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2f6a03811c04a5a849cd2c915319e2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/728 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2910\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 728\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize data, remove all columns and give new ones\n",
    "tokenized_Hraf = Hraf.map(preprocess_data, batched=True, remove_columns=Hraf['train'].column_names)\n",
    "tokenized_Hraf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "[CLS] the following episode was reported to me : a man had just discovered some breadfruits, which he was about to cook. a hungry woman, who was just passing by, took one. as soon as the man noticed the loss, he ran after the woman, grabbed her by the hair, and cut off her head, saying : â€œ now you do not need anything to eat any more. â€ [SEP]\n"
     ]
    }
   ],
   "source": [
    "# sample decoding\n",
    "example = tokenized_Hraf['train'][1]\n",
    "print(example.keys())\n",
    "print(tokenizer.decode(example['input_ids']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 0.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['EVENT', 'CAUSE']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(example['labels'])\n",
    "[id2label[idx] for idx, label in enumerate(example['labels']) if label == 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Truncated:  17\n",
      "Percentage Truncated: 0.6%\n",
      "[113, 161, 304, 322, 325, 546, 553, 633, 864, 1036, 1266, 1299, 2153, 2394, 2461, 2491, 2542]\n"
     ]
    }
   ],
   "source": [
    "# Number of passages longer than 512 tokens (and therefore truncated)\n",
    "sequence_i = []\n",
    "for i, tx in enumerate(tokenized_Hraf['train']):\n",
    "    if len(tx['input_ids']) >= 512:\n",
    "        sequence_i.append(i)\n",
    "print('Number Truncated: ', len(sequence_i))\n",
    "print(f'Percentage Truncated: {round(len(sequence_i)/len(tokenized_Hraf[\"train\"])*100,1)}%')\n",
    "print(sequence_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Stratification using multilabels is a difficult process as the number of unique bins of stratification increases exponentially by the number of labels (see more info and potential ways to conduct multilabel sttratification sampling <a href=\"https://dl.acm.org/doi/10.5555/2034161.2034172\"> HERE  </a>). We will currently disregard focusing on stratification of all the labels/classifications and just use a single label for stratification. Currently, this is still giving decent splits that do not deviate far from the true proportion or between n_splits. Still, one should check the proportional deviation of each label to make sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVENT:  0.618127147766323\n",
      "CAUSE:  0.46692439862542956\n",
      "ACTION: 0.4845360824742268\n",
      "\n",
      "EVENT:  0.6176975945017182\n",
      "CAUSE:  0.4677835051546392\n",
      "ACTION: 0.4845360824742268\n",
      "\n",
      "EVENT:  0.6194158075601375\n",
      "CAUSE:  0.47680412371134023\n",
      "ACTION: 0.4845360824742268\n",
      "\n",
      "EVENT:  0.6134020618556701\n",
      "CAUSE:  0.46606529209621994\n",
      "ACTION: 0.4845360824742268\n",
      "\n",
      "EVENT:  0.6207044673539519\n",
      "CAUSE:  0.4695017182130584\n",
      "ACTION: 0.4845360824742268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Splitting\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# folds = StratifiedKFold(n_splits=5)\n",
    "folds = StratifiedKFold(n_splits=5, shuffle= True, random_state=10)\n",
    "splits = folds.split(np.zeros(Hraf['train'].num_rows), Hraf['train']['ACTION'])\n",
    "\n",
    "\n",
    "train_list = []\n",
    "val_list = []\n",
    "\n",
    "for train_idxs, val_idxs in splits:\n",
    "    train_list += [train_idxs]\n",
    "    val_list += [val_idxs]\n",
    "    print(f\"EVENT:  {np.mean(Hraf['train'][train_idxs]['EVENT'])}\\nCAUSE:  {np.mean(Hraf['train'][train_idxs]['CAUSE'])}\\nACTION: {np.mean(Hraf['train'][train_idxs]['ACTION'])}\\n\")\n",
    "    \n",
    "# print(train_list,\"\\n\", val_list)\n",
    "# print(train_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def totalProportion(df, col, present=1):\n",
    "    value_counts = df[col].value_counts()\n",
    "    percentage = round(value_counts[present]/len(df)*100,2)\n",
    "    return percentage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a batch of examples using <a href=\"https://huggingface.co/docs/transformers/v4.29.0/en/main_classes/data_collator#transformers.DataCollatorWithPadding\"> DataCollatorWithPadding</a>. Itâ€™s more efficient to dynamically pad the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set tokenized passages to PyTorch Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_Hraf.set_format(\"torch\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain F1 score for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "    \n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Train\n",
    "Before you start training your model, create a map of the expected ids to their labels with id2label and label2id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", \n",
    "    problem_type='multi_label_classification',\n",
    "    num_labels = len(labels), \n",
    "    id2label=id2label, \n",
    "    label2id=label2id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.6919, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), logits=tensor([[-0.1087, -0.0794,  0.0418]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#forward pass TEST not implemented\n",
    "outputs = model(input_ids=tokenized_Hraf['train']['input_ids'][0].unsqueeze(0), labels=tokenized_Hraf['train'][0]['labels'].unsqueeze(0))\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/NLP-3/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2328\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1455\n",
      "  Number of trainable parameters = 66955779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Fold 1--------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ecf5ba8c564461da9d20912912e0168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1455 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 582\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65ec82745cc14ac19394a46f02380224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-291\n",
      "Configuration saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-291/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.46908506751060486, 'eval_f1': 0.7954319761668321, 'eval_roc_auc': 0.7579653121381198, 'eval_accuracy': 0.5343642611683849, 'eval_runtime': 119.5371, 'eval_samples_per_second': 4.869, 'eval_steps_per_second': 0.611, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-291/pytorch_model.bin\n",
      "tokenizer config file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-291/tokenizer_config.json\n",
      "Special tokens file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-291/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4767, 'learning_rate': 1.3127147766323025e-05, 'epoch': 1.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 582\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa756f0c5064a0bb7390b01a69cbc80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-582\n",
      "Configuration saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-582/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.48564615845680237, 'eval_f1': 0.8107302533532041, 'eval_roc_auc': 0.7758027160753763, 'eval_accuracy': 0.5721649484536082, 'eval_runtime': 118.0326, 'eval_samples_per_second': 4.931, 'eval_steps_per_second': 0.618, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-582/pytorch_model.bin\n",
      "tokenizer config file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-582/tokenizer_config.json\n",
      "Special tokens file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-582/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 582\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "878fda5e27a74d79b18bedbd68e125c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-873\n",
      "Configuration saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-873/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4657793641090393, 'eval_f1': 0.8146157979580871, 'eval_roc_auc': 0.8011817033371934, 'eval_accuracy': 0.5962199312714777, 'eval_runtime': 117.8688, 'eval_samples_per_second': 4.938, 'eval_steps_per_second': 0.619, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-873/pytorch_model.bin\n",
      "tokenizer config file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-873/tokenizer_config.json\n",
      "Special tokens file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-873/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3016, 'learning_rate': 6.254295532646049e-06, 'epoch': 3.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 582\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc1f765895b4e87b1702dc236edf6f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1164\n",
      "Configuration saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1164/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.47665542364120483, 'eval_f1': 0.8244596731681604, 'eval_roc_auc': 0.8069612590799031, 'eval_accuracy': 0.5979381443298969, 'eval_runtime': 117.1784, 'eval_samples_per_second': 4.967, 'eval_steps_per_second': 0.623, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1164/pytorch_model.bin\n",
      "tokenizer config file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1164/tokenizer_config.json\n",
      "Special tokens file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1164/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 582\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf33c27b45c6459ea0b81f628b1ba618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1455\n",
      "Configuration saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1455/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.48370838165283203, 'eval_f1': 0.8212765957446809, 'eval_roc_auc': 0.8057637646068008, 'eval_accuracy': 0.6030927835051546, 'eval_runtime': 117.3722, 'eval_samples_per_second': 4.959, 'eval_steps_per_second': 0.622, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1455/pytorch_model.bin\n",
      "tokenizer config file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1455/tokenizer_config.json\n",
      "Special tokens file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1455/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1164 (score: 0.8244596731681604).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 582\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 8387.3105, 'train_samples_per_second': 1.388, 'train_steps_per_second': 0.173, 'train_loss': 0.32988450617315024, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f472461a33b34c23b9354fd4e5fb0293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/NLP-3/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2328\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1455\n",
      "  Number of trainable parameters = 66955779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Fold 2--------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80f0cecf1eb44d8bad0da8aa7890e418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1455 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 582\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfead8d9283940f8b4338545abc73b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-291\n",
      "Configuration saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-291/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22418218851089478, 'eval_f1': 0.9173553719008265, 'eval_roc_auc': 0.9051410962707216, 'eval_accuracy': 0.7680412371134021, 'eval_runtime': 112.3707, 'eval_samples_per_second': 5.179, 'eval_steps_per_second': 0.65, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-291/pytorch_model.bin\n",
      "tokenizer config file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-291/tokenizer_config.json\n",
      "Special tokens file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-291/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2524, 'learning_rate': 1.3127147766323025e-05, 'epoch': 1.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 582\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f739b2f924b248fd9e8742200503e590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-582\n",
      "Configuration saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-582/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18772006034851074, 'eval_f1': 0.9403546480386888, 'eval_roc_auc': 0.9355530760658041, 'eval_accuracy': 0.8316151202749141, 'eval_runtime': 112.0601, 'eval_samples_per_second': 5.194, 'eval_steps_per_second': 0.651, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-582/pytorch_model.bin\n",
      "tokenizer config file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-582/tokenizer_config.json\n",
      "Special tokens file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-582/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 582\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0baf4e8bd84e56aef01c63b5990dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-873\n",
      "Configuration saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-873/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20057198405265808, 'eval_f1': 0.9265715795034336, 'eval_roc_auc': 0.9185033677055524, 'eval_accuracy': 0.7989690721649485, 'eval_runtime': 112.4741, 'eval_samples_per_second': 5.175, 'eval_steps_per_second': 0.649, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-873/pytorch_model.bin\n",
      "tokenizer config file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-873/tokenizer_config.json\n",
      "Special tokens file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-873/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1288, 'learning_rate': 6.254295532646049e-06, 'epoch': 3.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 582\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58743121689a43b585c1d4fac045e029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1164\n",
      "Configuration saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1164/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18837898969650269, 'eval_f1': 0.9354838709677419, 'eval_roc_auc': 0.9304143481756233, 'eval_accuracy': 0.8247422680412371, 'eval_runtime': 112.1873, 'eval_samples_per_second': 5.188, 'eval_steps_per_second': 0.651, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1164/pytorch_model.bin\n",
      "tokenizer config file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1164/tokenizer_config.json\n",
      "Special tokens file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1164/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 582\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f85dac80bfc41e59139f22eee84a154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1455\n",
      "Configuration saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1455/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2014915496110916, 'eval_f1': 0.9363057324840764, 'eval_roc_auc': 0.92968804480976, 'eval_accuracy': 0.8281786941580757, 'eval_runtime': 112.5879, 'eval_samples_per_second': 5.169, 'eval_steps_per_second': 0.648, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1455/pytorch_model.bin\n",
      "tokenizer config file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1455/tokenizer_config.json\n",
      "Special tokens file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1455/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-582 (score: 0.9403546480386888).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 582\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 8221.3157, 'train_samples_per_second': 1.416, 'train_steps_per_second': 0.177, 'train_loss': 0.1527593619225361, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dcb4e8832f14e1baaa146c1206cab22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/NLP-3/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2328\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1455\n",
      "  Number of trainable parameters = 66955779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Fold 3--------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53735e1ca0914c0090059f797b334099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1455 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 582\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb47398a99747639ca45be3d95e6df4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-291\n",
      "Configuration saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-291/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0940331518650055, 'eval_f1': 0.9720670391061453, 'eval_roc_auc': 0.9713189652456123, 'eval_accuracy': 0.9192439862542955, 'eval_runtime': 118.7081, 'eval_samples_per_second': 4.903, 'eval_steps_per_second': 0.615, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-291/pytorch_model.bin\n",
      "tokenizer config file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-291/tokenizer_config.json\n",
      "Special tokens file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-291/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1442, 'learning_rate': 1.3127147766323025e-05, 'epoch': 1.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 582\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf97cc3da5d46fd9677512f9c8dc691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-582\n",
      "Configuration saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-582/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.09501589089632034, 'eval_f1': 0.9705063995548137, 'eval_roc_auc': 0.969503261177805, 'eval_accuracy': 0.915807560137457, 'eval_runtime': 119.3632, 'eval_samples_per_second': 4.876, 'eval_steps_per_second': 0.612, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-582/pytorch_model.bin\n",
      "tokenizer config file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-582/tokenizer_config.json\n",
      "Special tokens file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-582/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 582\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf95389915c4a888d4a864e0cd67a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-873\n",
      "Configuration saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-873/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.10037727653980255, 'eval_f1': 0.9704735376044568, 'eval_roc_auc': 0.9695308315215679, 'eval_accuracy': 0.9175257731958762, 'eval_runtime': 118.9823, 'eval_samples_per_second': 4.891, 'eval_steps_per_second': 0.614, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-873/pytorch_model.bin\n",
      "tokenizer config file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-873/tokenizer_config.json\n",
      "Special tokens file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-873/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0655, 'learning_rate': 6.254295532646049e-06, 'epoch': 3.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 582\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb09afd54cf424297687353a5d8da39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1164\n",
      "Configuration saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1164/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.10411866009235382, 'eval_f1': 0.9707666850523994, 'eval_roc_auc': 0.9692826984277026, 'eval_accuracy': 0.9175257731958762, 'eval_runtime': 118.5987, 'eval_samples_per_second': 4.907, 'eval_steps_per_second': 0.616, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1164/pytorch_model.bin\n",
      "tokenizer config file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1164/tokenizer_config.json\n",
      "Special tokens file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1164/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 582\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5301d7b6bd640fbaa3376da0d6de314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1455\n",
      "Configuration saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1455/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.09135962277650833, 'eval_f1': 0.9750415973377704, 'eval_roc_auc': 0.9740051044522167, 'eval_accuracy': 0.929553264604811, 'eval_runtime': 118.7879, 'eval_samples_per_second': 4.899, 'eval_steps_per_second': 0.615, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1455/pytorch_model.bin\n",
      "tokenizer config file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1455/tokenizer_config.json\n",
      "Special tokens file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1455/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1455 (score: 0.9750415973377704).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 582\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 8159.8605, 'train_samples_per_second': 1.426, 'train_steps_per_second': 0.178, 'train_loss': 0.08234855350350187, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7131c528245d4acd968f67fefea51f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/NLP-3/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2328\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1455\n",
      "  Number of trainable parameters = 66955779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Fold 4--------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ac4630a23d04f77b0966b91f175977b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1455 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 582\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628c9dc5d039493dac9b0610a1b2e53f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-291\n",
      "Configuration saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-291/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.018161866813898087, 'eval_f1': 0.9935897435897435, 'eval_roc_auc': 0.9928572276076459, 'eval_accuracy': 0.979381443298969, 'eval_runtime': 115.067, 'eval_samples_per_second': 5.058, 'eval_steps_per_second': 0.634, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-291/pytorch_model.bin\n",
      "tokenizer config file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-291/tokenizer_config.json\n",
      "Special tokens file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-291/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0612, 'learning_rate': 1.3127147766323025e-05, 'epoch': 1.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 582\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6188054713c24a1084cda980864576ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-582\n",
      "Configuration saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-582/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.01046363078057766, 'eval_f1': 0.9983931440814141, 'eval_roc_auc': 0.9982340820192768, 'eval_accuracy': 0.9948453608247423, 'eval_runtime': 115.717, 'eval_samples_per_second': 5.03, 'eval_steps_per_second': 0.631, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-582/pytorch_model.bin\n",
      "tokenizer config file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-582/tokenizer_config.json\n",
      "Special tokens file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-582/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 582\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f03be36b144dbb9c26d325428e44f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-873\n",
      "Configuration saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-873/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.009403296746313572, 'eval_f1': 0.9962506695232994, 'eval_roc_auc': 0.9959322583579533, 'eval_accuracy': 0.9879725085910653, 'eval_runtime': 115.3099, 'eval_samples_per_second': 5.047, 'eval_steps_per_second': 0.633, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-873/pytorch_model.bin\n",
      "tokenizer config file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-873/tokenizer_config.json\n",
      "Special tokens file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-873/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0279, 'learning_rate': 6.254295532646049e-06, 'epoch': 3.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 582\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b5b92dfad34301a038eb4c7b6fe853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1164\n",
      "Configuration saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1164/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.008633813820779324, 'eval_f1': 0.9973161567364467, 'eval_roc_auc': 0.9972413711275376, 'eval_accuracy': 0.9914089347079038, 'eval_runtime': 115.1604, 'eval_samples_per_second': 5.054, 'eval_steps_per_second': 0.634, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1164/pytorch_model.bin\n",
      "tokenizer config file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1164/tokenizer_config.json\n",
      "Special tokens file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1164/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 582\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b751ba4cd107404e80208f9be023816c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1455\n",
      "Configuration saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1455/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.008361565880477428, 'eval_f1': 0.9967880085653105, 'eval_roc_auc': 0.9964681640385534, 'eval_accuracy': 0.9896907216494846, 'eval_runtime': 115.5381, 'eval_samples_per_second': 5.037, 'eval_steps_per_second': 0.632, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1455/pytorch_model.bin\n",
      "tokenizer config file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1455/tokenizer_config.json\n",
      "Special tokens file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1455/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-582 (score: 0.9983931440814141).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 582\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 8218.5695, 'train_samples_per_second': 1.416, 'train_steps_per_second': 0.177, 'train_loss': 0.03276498555317777, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb9dcad83964b909c88e2a0af6b141e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/NLP-3/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2328\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1455\n",
      "  Number of trainable parameters = 66955779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Fold 5--------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcbda37c2c624dc69907e2164804d7a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1455 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 582\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a46c0452654ed1820a20aa10f2e7eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-291\n",
      "Configuration saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-291/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.012662551365792751, 'eval_f1': 0.996138996138996, 'eval_roc_auc': 0.996053378612808, 'eval_accuracy': 0.9879725085910653, 'eval_runtime': 116.0117, 'eval_samples_per_second': 5.017, 'eval_steps_per_second': 0.629, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-291/pytorch_model.bin\n",
      "tokenizer config file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-291/tokenizer_config.json\n",
      "Special tokens file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-291/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0349, 'learning_rate': 1.3127147766323025e-05, 'epoch': 1.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 582\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ae5933853947a5a08bbc4e93ebeba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.013464166782796383, 'eval_f1': 0.9950467804072647, 'eval_roc_auc': 0.994814063272108, 'eval_accuracy': 0.9845360824742269, 'eval_runtime': 115.9343, 'eval_samples_per_second': 5.02, 'eval_steps_per_second': 0.63, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-582/config.json\n",
      "Model weights saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-582/pytorch_model.bin\n",
      "tokenizer config file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-582/tokenizer_config.json\n",
      "Special tokens file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-582/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 582\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c65266d67f4db7ab1ac9d37a1e1d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.010247824713587761, 'eval_f1': 0.9983489268024216, 'eval_roc_auc': 0.998256021784614, 'eval_accuracy': 0.9948453608247423, 'eval_runtime': 115.699, 'eval_samples_per_second': 5.03, 'eval_steps_per_second': 0.631, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-873/config.json\n",
      "Model weights saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-873/pytorch_model.bin\n",
      "tokenizer config file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-873/tokenizer_config.json\n",
      "Special tokens file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-873/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.016, 'learning_rate': 6.254295532646049e-06, 'epoch': 3.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 582\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "348a9999aa994fcfab0d4b0659ac7cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.005906226113438606, 'eval_f1': 0.9983471074380165, 'eval_roc_auc': 0.9983020197028798, 'eval_accuracy': 0.9948453608247423, 'eval_runtime': 115.6828, 'eval_samples_per_second': 5.031, 'eval_steps_per_second': 0.631, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1164/config.json\n",
      "Model weights saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1164/pytorch_model.bin\n",
      "tokenizer config file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1164/tokenizer_config.json\n",
      "Special tokens file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1164/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 582\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d233a4c83e549cabf622ca60669876d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.005445130169391632, 'eval_f1': 0.998898678414097, 'eval_roc_auc': 0.9988526804958313, 'eval_accuracy': 0.9965635738831615, 'eval_runtime': 115.6366, 'eval_samples_per_second': 5.033, 'eval_steps_per_second': 0.631, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1455/config.json\n",
      "Model weights saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1455/pytorch_model.bin\n",
      "tokenizer config file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1455/tokenizer_config.json\n",
      "Special tokens file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1455/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from HRAF_Model_MultiLabel_ThreeLargeClasses2/checkpoint-1455 (score: 0.998898678414097).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 582\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 8380.1935, 'train_samples_per_second': 1.389, 'train_steps_per_second': 0.174, 'train_loss': 0.01814969905053627, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c3ed2765e7a4cf0acb55e2505d47c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"HRAF_Model_MultiLabel_ThreeLargeClasses2\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    push_to_hub=False,\n",
    ")\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_Hraf[\"train\"],\n",
    "#     eval_dataset=tokenized_Hraf[\"train\"],\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=data_collator,\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )\n",
    "\n",
    "eval_df = pd.DataFrame()\n",
    "# # Train the model\n",
    "for fold, (train_idxs, val_idxs) in enumerate(zip(train_list, val_list)): # K-fold loop\n",
    "    print(f\"------Fold {fold+1}--------\\n\")\n",
    "    train_ds = tokenized_Hraf[\"train\"].select(train_idxs)\n",
    "    val_ds = tokenized_Hraf[\"train\"].select(val_idxs)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate and then concatinate results to a dataframe\n",
    "    eval_dict = trainer.evaluate()\n",
    "    eval_df_line = pd.DataFrame([eval_dict])\n",
    "    eval_df_line[\"Fold\"] = fold+1\n",
    "    eval_df = pd.concat([eval_df, eval_df_line])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model to local or to the hub (you could do both but that may be redundant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to HRAF_Model_MultiLabel_ThreeLargeClasses2\n",
      "Configuration saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/config.json\n",
      "Model weights saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/pytorch_model.bin\n",
      "tokenizer config file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/tokenizer_config.json\n",
      "Special tokens file saved in HRAF_Model_MultiLabel_ThreeLargeClasses2/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# Save the model to disk\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Repository.__init__() got an unexpected keyword argument 'private'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/ericchantland/Library/CloudStorage/Dropbox/MEM-DEV-LAB-Current/2023-eHRAF-Misf/HRAF-Misf-NaturalLanguageProcessing/HRAF_NLP/HRAF_MultiLabel_ThreeLargeClasses_DEMO/HRAF_Training_multiLabel_ThreeLargeClasses_DEMO.ipynb Cell 39\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ericchantland/Library/CloudStorage/Dropbox/MEM-DEV-LAB-Current/2023-eHRAF-Misf/HRAF-Misf-NaturalLanguageProcessing/HRAF_NLP/HRAF_MultiLabel_ThreeLargeClasses_DEMO/HRAF_Training_multiLabel_ThreeLargeClasses_DEMO.ipynb#Y112sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mpush_to_hub()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP-3/lib/python3.10/site-packages/transformers/trainer.py:3431\u001b[0m, in \u001b[0;36mTrainer.push_to_hub\u001b[0;34m(self, commit_message, blocking, **kwargs)\u001b[0m\n\u001b[1;32m   3428\u001b[0m \u001b[39m# If a user calls manually `push_to_hub` with `self.args.push_to_hub = False`, we try to create the repo but\u001b[39;00m\n\u001b[1;32m   3429\u001b[0m \u001b[39m# it might fail.\u001b[39;00m\n\u001b[1;32m   3430\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrepo\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 3431\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minit_git_repo()\n\u001b[1;32m   3433\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mshould_save:\n\u001b[1;32m   3434\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mhub_model_id \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP-3/lib/python3.10/site-packages/transformers/trainer.py:3284\u001b[0m, in \u001b[0;36mTrainer.init_git_repo\u001b[0;34m(self, at_init)\u001b[0m\n\u001b[1;32m   3281\u001b[0m     repo_name \u001b[39m=\u001b[39m get_full_repo_name(repo_name, token\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mhub_token)\n\u001b[1;32m   3283\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3284\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepo \u001b[39m=\u001b[39m Repository(\n\u001b[1;32m   3285\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49moutput_dir,\n\u001b[1;32m   3286\u001b[0m         clone_from\u001b[39m=\u001b[39;49mrepo_name,\n\u001b[1;32m   3287\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   3288\u001b[0m         private\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mhub_private_repo,\n\u001b[1;32m   3289\u001b[0m     )\n\u001b[1;32m   3290\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[1;32m   3291\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39moverwrite_output_dir \u001b[39mand\u001b[39;00m at_init:\n\u001b[1;32m   3292\u001b[0m         \u001b[39m# Try again after wiping output_dir\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP-3/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: Repository.__init__() got an unexpected keyword argument 'private'"
     ]
    }
   ],
   "source": [
    "# Push to hub (I have not gotten this to work so alternatively you can manually add in the best checkpoint by uploading the checkpoint into your hugging face account)\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Evaluation Dataset columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fold</th>\n",
       "      <th>epoch</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_roc_auc</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.476655</td>\n",
       "      <td>0.824460</td>\n",
       "      <td>0.806961</td>\n",
       "      <td>0.597938</td>\n",
       "      <td>117.8543</td>\n",
       "      <td>4.938</td>\n",
       "      <td>0.619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.187720</td>\n",
       "      <td>0.940355</td>\n",
       "      <td>0.935553</td>\n",
       "      <td>0.831615</td>\n",
       "      <td>114.2153</td>\n",
       "      <td>5.096</td>\n",
       "      <td>0.639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.091360</td>\n",
       "      <td>0.975042</td>\n",
       "      <td>0.974005</td>\n",
       "      <td>0.929553</td>\n",
       "      <td>120.2193</td>\n",
       "      <td>4.841</td>\n",
       "      <td>0.607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.010464</td>\n",
       "      <td>0.998393</td>\n",
       "      <td>0.998234</td>\n",
       "      <td>0.994845</td>\n",
       "      <td>117.1711</td>\n",
       "      <td>4.967</td>\n",
       "      <td>0.623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.005445</td>\n",
       "      <td>0.998899</td>\n",
       "      <td>0.998853</td>\n",
       "      <td>0.996564</td>\n",
       "      <td>116.6169</td>\n",
       "      <td>4.991</td>\n",
       "      <td>0.626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fold  epoch  eval_loss   eval_f1  eval_roc_auc  eval_accuracy   \n",
       "0     1    5.0   0.476655  0.824460      0.806961       0.597938  \\\n",
       "0     2    5.0   0.187720  0.940355      0.935553       0.831615   \n",
       "0     3    5.0   0.091360  0.975042      0.974005       0.929553   \n",
       "0     4    5.0   0.010464  0.998393      0.998234       0.994845   \n",
       "0     5    5.0   0.005445  0.998899      0.998853       0.996564   \n",
       "\n",
       "   eval_runtime  eval_samples_per_second  eval_steps_per_second  \n",
       "0      117.8543                    4.938                  0.619  \n",
       "0      114.2153                    5.096                  0.639  \n",
       "0      120.2193                    4.841                  0.607  \n",
       "0      117.1711                    4.967                  0.623  \n",
       "0      116.6169                    4.991                  0.626  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reorganize columns\n",
    "cols = list(eval_df.columns.values) \n",
    "cols = [cols[-1]] + [cols[-2]] + cols[:-2]\n",
    "eval_df = eval_df[cols]\n",
    "\n",
    "eval_df.to_excel(\"Evaluation.xlsx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = \"If one falls sick or dies, the natives at once conclude he must have been bewitched, or bitten, or hurt by the devil-- eringa .\"\n",
    "\n",
    "encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n",
    "\n",
    "outputs = trainer.model(**encoding)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logits that come out of the model are of shape (batch_size, num_labels). As we are only forwarding a single sentence through the model, the `batch_size` equals 1. The logits is a tensor that contains the (unnormalized) scores for every individual label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "logits = outputs.logits\n",
    "logits.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To turn them into actual predicted labels, we first apply a sigmoid function independently to every score, such that every score is turned into a number between 0 and 1, that can be interpreted as a \"probability\" for how certain the model is that a given class belongs to the input text.\n",
    "\n",
    "Next, we use a threshold (typically, 0.5) to turn every probability into either a 1 (which means, we predict the label for the given example) or a 0 (which means, we don't predict the label for the given example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EVENT', 'CAUSE']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# apply sigmoid + threshold\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "probs = sigmoid(logits.squeeze().cpu())\n",
    "predictions = np.zeros(probs.shape)\n",
    "predictions[np.where(probs >= 0.5)] = 1\n",
    "# turn predicted id's into actual label names\n",
    "predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy k-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/ericchantland/Library/CloudStorage/Dropbox/MEM-DEV-LAB-Current/2023-eHRAF-Misf/HRAF-Misf-NaturalLanguageProcessing/HRAF_NLP/HRAF_MultiLabel_ThreeLargeClasses_DEMO/HRAF_Training_multiLabel_ThreeLargeClasses_DEMO.ipynb Cell 52\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ericchantland/Library/CloudStorage/Dropbox/MEM-DEV-LAB-Current/2023-eHRAF-Misf/HRAF-Misf-NaturalLanguageProcessing/HRAF_NLP/HRAF_MultiLabel_ThreeLargeClasses_DEMO/HRAF_Training_multiLabel_ThreeLargeClasses_DEMO.ipynb#Y153sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ericchantland/Library/CloudStorage/Dropbox/MEM-DEV-LAB-Current/2023-eHRAF-Misf/HRAF-Misf-NaturalLanguageProcessing/HRAF_NLP/HRAF_MultiLabel_ThreeLargeClasses_DEMO/HRAF_Training_multiLabel_ThreeLargeClasses_DEMO.ipynb#Y153sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39mtrain.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ericchantland/Library/CloudStorage/Dropbox/MEM-DEV-LAB-Current/2023-eHRAF-Misf/HRAF-Misf-NaturalLanguageProcessing/HRAF_NLP/HRAF_MultiLabel_ThreeLargeClasses_DEMO/HRAF_Training_multiLabel_ThreeLargeClasses_DEMO.ipynb#Y153sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m data[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m data[\u001b[39m\"\u001b[39m\u001b[39msentiment\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ericchantland/Library/CloudStorage/Dropbox/MEM-DEV-LAB-Current/2023-eHRAF-Misf/HRAF-Misf-NaturalLanguageProcessing/HRAF_NLP/HRAF_MultiLabel_ThreeLargeClasses_DEMO/HRAF_Training_multiLabel_ThreeLargeClasses_DEMO.ipynb#Y153sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train, validation \u001b[39m=\u001b[39m train_test_split(data, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP-3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP-3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP-3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP-3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP-3/lib/python3.10/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data = pd.read_excel(f\"{path}{dataFolder}/_Altogether_Dataset_RACoded.xlsx\", header=[0,1], index_col=0)\n",
    "data[\"label\"] = data[\"EVENT\"]\n",
    "train, validation = train_test_split(data, test_size=0.2)\n",
    "train.to_csv(\"new_train.csv\")\n",
    "validation.to_csv(\"validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = Dataset.load_dataset(\"csv\", data_files={\"validation\": \"validation.csv\"}, split=[f\"validation[{k}%:{k+10}%]\" for k in range(0, 100, 10)])\n",
    "train_ds = Dataset.load_dataset(\"csv\", data_files={\"train\": \"new_train.csv\"}, split=[f\"train[:{k}%]+train[{k+10}%:]\" for k in range(0, 100, 10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Tokenize the texts\n",
    "    args = ((examples[\"review\"],))\n",
    "    result = tokenizer(*args, padding=True, max_length=128, truncation=True)\n",
    "    result[\"label\"] = examples[\"label\"]\n",
    "    return result\n",
    "\n",
    "for idx, item in enumerate(train_ds):\n",
    "    train_ds[idx] = train_ds[idx].map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "\n",
    "for idx, item in enumerate(val_ds):\n",
    "    val_ds[idx] = val_ds[idx].map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_dataset, val_dataset in zip(train_ds, val_ds):\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
